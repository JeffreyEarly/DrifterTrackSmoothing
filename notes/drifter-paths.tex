\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{tensor}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Notes on modeling a good drifter path from noisy data}
\author{Jeffrey J. Early}
\date{August 5th, 2015}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Introduction}

The raw global drifter dataset is a set of unevenly sampled position values (latitude and longitude) from Lagrangian floats. From this dataset we may want to extract a few key pieces of information:
\begin{itemize}
	\item An estimate of the drifter's path, maybe at regular time intervals.
	\item An estimate the drifter's velocity at each time point.
	\item An estimate of the total force vector acting on the drifter at each time point (or just a sum of the work on the drifter between intervals).
\end{itemize}

The noise in position locations from ARGOS can vary from a few hundred meters to a few kilometers, with sample times typically varying from about half an hour to several hours. One consequence of this is that even a motionless drifter would appear to have a nonzero velocity proportional to position noise ($dx$) and inversely proportional to the sample time interval ($dt$). While the drifters tracked with GPS have smaller errors, they fundamentally suffer from the same problem and should be treated in the same way.

What I'm going to argue is that we can use our knowledge of the underlying physics to improve the quality of fits. In particular, we take ideas from the path integral formulation of quantum and classical mechanics to improve our maximum likelihood estimates. To continue, we first need to review maximum likelihood and path integrals.

Throughout this discussion assume that we have collected bivariate time series data of drifter positions given as either projected coordinates $(x_i, y_i)$ or longitude/latitude $(\phi_i, \theta_i$) at times $t_i$. Our goal is to create a model of position $(x(t),y(t))$ that is continuous in $t$, and perhaps even continuous in the first derivative (velocity) that best matches the data.

\section{Maximum Likelihood}

The following discussion is largely based off of my reading of the Numerical Recipes Chapter 15 on ``Modeling of Data''.

Slightly rewording a quote from Numerical Recipes, the central idea of maximum likelihood is to ask ``Given a particular path $(x(t),y(t))$, what is the probability that this dataset $(t_i,x_i, y_i)$ could have occurred?'' The goal is then to find the path that is most likely to have produced that dataset.

Conceptually we have two major pieces that we need to solve this problem:
\begin{enumerate}
\item we need to specify the probability function and
\item we need to specify the form of the path (model).
\end{enumerate}

The canonical example in one-dimension is to assume that the error in our position measurements are Gaussian and therefore the probability of the observed data given the model is,
\begin{equation}
P \sim \prod \exp \left[ -\frac{1}{2} \left( \frac{x_i - x(t_i)}{\sigma_i} \right)^2 \right] \Delta x
\end{equation}
where $x_i$ represents the observations at time $t_i$ with estimated error of $\sigma_i$. In general, our data may not be Gaussian, and the observations may not be truly independent (meaning that points nearby in time should be weighted to reflect that the drifter didn't likely move very far).

The form given to the path $x(t)$ shouldn't be too important, as long as it doesn't have too many free parameters. For example, you don't want to be fitting a quadratic polynomial to only two data points because it's under-specified---but it is perfectly fine to fit a quadratic polynomial to, say, ten data points. I would argue that if the answer you get depends strongly on the form of the model, then you're doing something wrong.

\section{Path Integrals}

To determine the path that a particle takes in classical mechanics, one typically applies Newton's second law,
\begin{equation}
m\ddot{x} = F(x,t).
\end{equation}
Once the forces $F(x,t)$ have been specified, then, in principal, it is possible to find the path $x(t)$ that the particle must have taken. If the forces are conservative, then they can be written as the derivative of potential energy, $F(x,t) = - \frac{\partial V(x,t)}{\partial x}$.

A useful alternative to directly applying Newton's second law, is to instead find the path $x(t)$ that \emph{minimize the action}
\begin{equation}
S = \int \mathcal{L} \, dt
\end{equation}
where $\mathcal{L}=\frac{1}{2} m \dot{x}^2 - V(x)$. The terminology here is that the difference between the kinetic energy and potential energy, $\mathcal{L}$, is the \emph{Lagrangian} while its time integral, $S$, is the \emph{action}.

To find the path that minimizes the action $S$, one could simply try a number of different paths $x_\alpha(t)$ and use a minimization search algorithm to find the best path. Of course, this can also be done analytically, where the action is minimized by applying the Euler-Lagrange equations,
\begin{equation}
\frac{d}{dt} \left( \frac{ \partial \mathcal{L}}{\partial \dot{x}} \right) = \frac{\partial \mathcal{L}}{\partial x}
\end{equation}
which can be seen to reproduce Newton's second law. \textbf{This means that we can understand nature as choosing the path that minimizes the time averaged difference between the kinetic and potential energy.} In the absence of potential energy, this results in a path is simply a straight line.

The principal of least-action doesn't deal with non-conservative forces (such as friction), but it does allow one to specify constraints amongst the variables (known holonomic constraints), which allows us to constrain the particle to lie along a particular path. One possible constraint would be to say that the path must cross a particular point at a particular time, that is $x(t_0)=x_0$. This constraint should look a lot like observed data of a particle path. Added to the Lagrangian this takes the form
\begin{equation}
\mathcal{L}=\frac{1}{2} m \dot{x}^2 - V(x) + \lambda^{(0)}(x_0-x(t_0))
\end{equation}
where $\lambda^{(0)}$ is now treated as a coordinate variable and the constraint is recovered by applying the Euler-Lagrange equations. If one had several such constraints,
\begin{equation}
\label{constrained_lagrangian}
\mathcal{L}=\frac{1}{2} m \dot{x}^2 - V(x) + \sum_{i=0}^N \lambda^{(i)}(x_i-x(t_i)).
\end{equation}
At this point, one could simply divide up the action into a series of separate sums,
\begin{equation}
S = \sum_{i=0}^{N-1} \int_{t_i}^{t_{i+1}} \frac{1}{2} m \dot{x}^2 - V(x)
\end{equation}
where the final and initial conditions of each integral are the observed locations of the particle. In the absence of a potential $V(x)$, this would simply be a series of straight lines between the points. This is a useful result insofar as it tells us that in the absence of additional information, it's reasonable to assume that particles travel in straight lines between observations.

\subsection{Quantum mechanics}

Taking this all one step further, in quantum mechanics one allows \emph{all} possible paths, including the classical, to exist. The probability amplitude of a particle traverse between two points is the sum of all those paths,
\begin{equation}
P = \int \exp\left[ \frac{i}{\hbar} S \right] \delta x
\end{equation}
where $\hbar$ is simply a normalization with units Joules-seconds. The odd part is perhaps the $i$, which associates a phase with each path---so each path has the same amplitude, but different phase. The integral $\int \delta x$ is a sum over all paths $x$. In a more discrete form this looks like,
\begin{equation}
P = \prod \exp\left[ \frac{i}{\hbar} \int \mathcal{L} \right] Dx
\end{equation}
The interpretation here is very interesting. The Euler-Lagrange equations give use the \emph{classical} path that minimizes $S$, call it $x_{\textrm{cl}}(t)$. Because $x_{\textrm{cl}}(t)$ is a minimum of $S$, other paths that are nearby will also have nearly the same phase (and therefore add constructively), but as soon as the phase wanders approximately $\pi$ away from the minimum, then other paths start to cancel each other out (and add de-constructively).

\section{Putting this all together}

From the section on maximum likelihood, we know we want to construct a path $x(t)$ that is most likely to exist given the observed data and known error of observations. On the other hand, we know from the section of path integrals, that nature chooses paths that minimize the time averaged difference between kinetic and potential energy. By combining both of these ideas, I think it is possible to construct paths that match the data and are physically realistic.

It's important to note that by adding this constraint on the velocity (and kinetic energy) even if you use some tenth order polynomial to fit the data, you will \emph{not} get spurious paths with huge accelerations. The constraint on velocities will prevent that.

\subsection{Non Gaussian distributions}

First, note that maximum likelihood does \emph{not} require we use Gaussian distributions---when we do, then maximum likelihood reduces to least-squares---but we can use other distributions that may better fit the data.

In the case of noisy measurements it may, in fact, be better to use a two-sided exponential (see section 15.7 in Numerical Recipes for `Robust Modelling'),
\begin{equation}
P \sim \exp \left[ -\left\lvert  \frac{x_i - x(t_i)}{\sigma_i} \right\rvert \right]
\end{equation}
because this has a much longer tail than a Gaussian.

\subsection{Principal of least action as a probability}

In the case of quantum mechanics the action was treated as a phase parameter where trajectories near the classical minimum contributed more the final probability. Instead of worrying about phase, for our purposes we should just say that paths that minimize the action are more probable than those that don't. In other words, eliminated the $i$ from the quantum mechanics definition so that, 
\begin{equation}
P \sim \exp \left[ \frac{S}{E} \right]
\end{equation}
where $E$ is some constant similar to $\hbar$ that has units of Joules-seconds.

\subsection{Constraining velocity}

The standard one-dimension action with a potential energy is simply,
\begin{equation}
S = \int \frac{1}{2} \dot{x}^2 \, dt.
\end{equation}
Let's say that our model for the trajectory of the particle is linear, e.g., $x(t) = u t + b$. In this case then, the principal of least action is equivalent to minimizing the velocity. That is,
\begin{equation}
P \sim \exp \left[ -  \frac{u^2}{E} \Delta t \right]
\end{equation}
which is equivalent to just saying,
\begin{equation}
P \sim \exp \left[ -  \frac{u^2}{\sigma_u^2} \right]
\end{equation}
where $\sigma_u$ is some velocity variance.

BUT, now that we've gotten here, it's worth noting that we know something about the velocity PDFs in the ocean, see the two papers of Bracco et al., 2000. In fact, the fit is not really a Gaussian, but is again more like a two-sided exponential.

This is an argument then that the most likely path might take something of the form,
\begin{equation}
P \sim \prod \exp \left[ -\frac{1}{2} \left( \frac{x_i - x(t_i)}{\sigma_i} \right)^2 -\left\lvert  \frac{\dot{x}}{\sigma_u} \right\rvert \right] \Delta x.
\end{equation}
This isn't quite right, and this assumes that we have a linear model. As soon as we have a nonlinear model (e.g., $x(t)=at^2+bt+c$), then we really will need to include the integral over the kinetic energy proper.

\subsection{Including the geopotential}

Up until now we consider the Lagrangian for a free particle in an inertial frame. BUT, if we consider the Lagrangian for a particle constrained to the surface of the earth, then
\begin{equation}
\mathcal{L} = \frac{1}{2}R^2 \dot{\theta}^2 + \frac{1}{2} R^2 \left( \dot{\phi}^2 + 2 \omega \dot{\phi} \right) \cos^2 \theta.
\end{equation}
Note that $u = R \dot{\phi} \cos \theta$ and $v=R\dot{\theta}$ which means this is just,
\begin{equation}
\mathcal{L} =  \frac{1}{2} u^2  + \frac{1}{2} v^2 +  u \omega R \cos \theta.
\end{equation}
This extra term on the end is a potential energy term, and is exactly what causes inertial oscillations.

A key point in Early (2012) is that the `straight lines' (geodesics) on the earth are actually inertial oscillations. So, if you think that the idea of minimizing the action has any merit whatsoever, then this modified Lagrangian is the one you should use. In other words don't minimize something that looks like this,
\begin{equation}
\phi = \frac{ [x_i - x(t_i) ]^2}{2 \sigma_i^2} + \frac{ [y_i - y(t_i) ]^2}{2 \sigma_i^2} + \frac{1}{2 E_0} \left[ u^2(t_i) + v^2(t_i) \right]
\end{equation}
instead minimize something that looks like this,
\begin{equation}
\phi = \frac{ [\theta_i -\theta(t_i) ]^2}{2 \sigma_i^2} + \frac{ [\phi_i - \phi(t_i) ]^2}{2 \sigma_i^2} + \frac{1}{ E_0}  \left[ u^2(t_i) + v^2(t_i) +  u \omega R \cos \theta \right]
\end{equation}
where $\omega$ is the rotation rate of the Earth.

My biggest objection to what's written above is that we don't want to evaluate the energies (velocities) just at times $t_i$, but instead we want the integral over the path length, as is seen in the action.

\section{Master Equation}

So, given all that we've argued the thing you REALLY want to maximize is $\exp(\phi)$ where,
\begin{equation}
\phi = \frac{ [\theta_i -\theta(t_i) ]^2}{2 \sigma_i^2} + \frac{ [\phi_i - \phi(t_i) ]^2}{2 \sigma_i^2} + \frac{S}{S_0} 
\end{equation}
and,
\begin{equation}
S = \int_{0}^{T} \left[ \frac{1}{2} u^2(t) + \frac{1}{2} v^2(t) +  u(t) \Omega R \cos \theta(t) \right]\, dt
\end{equation}
where $S_0$ is some characteristic scale. Compare these two equations to Teanby (2007) equations 27 and 31---they're almost identical and, in fact, in his approximation at equation 34, they are identical.

So first, what is $S_0$? The action $S$ is very nearly a measure of the time averaged kinetic energy of the particles, scaled by T. So this is equivalent to $S_0 = \frac{1}{2} \bar{v}^2 T$ where $\bar{v}$ is a characteristic velocity and $T$ is the total time of the integral.

\subsection{$f$-plane}

Let's simplify this to a Cartesian coordinates for the moment, projected on to some $f$-plane so that latitude appears fixed. Thus,
\begin{equation}
\phi = \frac{ [x_i -x(t_i) ]^2}{2 \sigma_i^2} + \frac{ [y_i - y(t_i) ]^2}{2 \sigma_i^2} + \frac{S}{S_0} 
\end{equation}
and,
\begin{equation}
S = \int_{0}^{T} \left[ \frac{1}{2} \left( \frac{dx}{dt} \right)^2 + \frac{1}{2} \left( \frac{dy}{dt} \right)^2 +  \left( \frac{dx}{dt} \right) f_0 y \right]\, dt.
\end{equation}
This approximation comes from equation (22) in Ripa (1997). Following the notation of Teanby (2007), we will write $x(t)$ and $y(t)$ in terms of the basis functions $B_j(t)$ where
\begin{align}
x(t) =& \sum_{j=1}^M \xi_j B_j(t) \\
y(t) =& \sum_{j=1}^M \eta_j B_j(t)
\end{align}
There are $M$ basis functions (in this case splines), each identical but centered on (M) different `knot' points. The goal is to determine the coefficients $\xi_j$ and $\eta_j$.

Let's say we have $N$ different observations of $(x_i, y_i)$ at times $t_i$. Then the model fit at these points is,
\begin{align}
x(t^i) =& \sum_{j=1}^M \xi_j B_j(t^i) \\
y(t^i) =& \sum_{j=1}^M \eta_j B_j(t^i)
\end{align}
In matrix notation,
\begin{equation}
x^i = B\indices{^i_j}\xi^j
\end{equation}
where the first index of $B$ references the $N$ observations at times $t^i$, the second index of $B$ references the $M$ knot points. Using this notation, the penalty function $\phi$ becomes,
\begin{equation}
\phi = \frac{1}{2} \left[ x^k - B\indices{^k_j} \xi^j \right]^{\textrm{T}} W\indices{^k_i} \left[ x^i - B\indices{^i_l} \xi^l \right] + \frac{1}{2} \left[ y^k - B\indices{^k_j} \eta^j \right]^{\textrm{T}} W\indices{^k_i} \left[ y^i - B\indices{^i_l} \eta^l \right] + S/S_0
\end{equation}
where the matrix $W$ is the inverse covariance matrix, e.g., $1/\sigma^2$ or something.

Finding the minimum of $\phi$ is best done by finding where its derivative vanishes. This is standard fare for the least-squares part of the equation. In particular,
\begin{align}
\bar{X}^2 \equiv&  \left[ x^k - B\indices{^k_j} \xi^j \right]^{\textrm{T}} W\indices{^k_i} \left[ x^i - B\indices{^i_l} \xi^l \right] \\
=&  \left[ x_k - B\indices{_k^j} \xi_j \right] W\indices{^k_i} \left[ x^i - B\indices{^i_l} \xi^l \right] \\
=&  \left[ x_k W\indices{^k_i} - \xi_j B\indices{_k^j} W\indices{^k_i} \right] \left[ x^i - B\indices{^i_l} \xi^l \right] \\
=& x_k W\indices{^k_i} x^i - \xi_j B\indices{_k^j} W\indices{^k_i} x^i - x_k W\indices{^k_i}  B\indices{^i_l} \xi^l+ \xi_j B\indices{_k^j} W\indices{^k_i}  B\indices{^i_l} \xi^l \\
=& x_k W\indices{^k_i} x^i - 2 x_k W\indices{^k_i}  B\indices{^i_j} \xi^j+ \xi_j B\indices{_k^j} W\indices{^k_i}  B\indices{^i_l} \xi^l.
\end{align}
Using that
\[
\frac{\partial{ \xi^m}}{\partial \xi^n} = \delta\indices{^m_n}.
\]
we can carefully differentiate the last term,
\begin{align}
&  \frac{\partial}{\partial \xi^m} \left[ \xi_j B\indices{_k^j} W\indices{^k_i}  B\indices{^i_l} \xi^l \right] \\
&=\frac{\partial}{\partial \xi^m} \left[ \xi^n \delta\indices{_j_n}  B\indices{_k^j} W\indices{^k_i}  B\indices{^i_l} \xi^l \right] \\
&= \delta\indices{^n_m} \delta\indices{_j_n}  B\indices{_k^j} W\indices{^k_i}  B\indices{^i_l} \xi^l + \xi_j B\indices{_k^j} W\indices{^k_i}  B\indices{^i_l} \delta\indices{^l_m} \\
&=  B\indices{_k_m} W\indices{^k_i}  B\indices{^i_l} \xi^l + \xi_j B\indices{_k^j} W\indices{^k_i}  B\indices{^i_m} \\
&= B\indices{_k_m} W\indices{^k_i}  B\indices{^i_l} \xi^l +  B\indices{^i_m} W\indices{^k_i}   B\indices{_k^j} \xi_j \\
&= B\indices{_k_m} W\indices{^k_i}  B\indices{^i_l} \xi^l +  B\indices{^k_m} W\indices{^k_i}   B\indices{_i^j} \xi_j
\end{align}
where we used the fact that $W$ is symmetric in the last step. With some simple raising and lowering, these two terms are identical. In total then,
\begin{align}
\frac{\partial \bar{X}^2}{\partial \xi^m} = - 2 x_k W\indices{^k_i}  B\indices{^i_m}+2 B\indices{_k_m} W\indices{^k_i}  B\indices{^i_l} \xi^l.
\end{align}

The action also needs to be computed, but it doesn't need to be on the same grid as the observations. Instead, we will create $Q$ evenly spaced points on our `quadrature grid', following Teanby (2007) where $Q = 5 T/k$ where $k$ is the spacing between the $M$ knots. Thus, in discrete form
\begin{equation}
S = \sum_{q=1}^Q \left[ \frac{1}{2} \left( {B^\prime}\indices{^q_j} \xi^j \right)^2 +  \frac{1}{2} \left( {B^\prime}\indices{^q_j} \eta^j \right)^2 -  f_0 {B^\prime}\indices{^q_j} \xi^j B\indices{^q_j}\eta^j \right] \Delta t
\end{equation}
Differentiating with respected to the parameters,
\begin{align}
\frac{\partial S}{\partial \xi^k} =& \sum_{q=1}^Q \left[ {B^\prime}\indices{^q_k} \left( {B^\prime}\indices{^q_j} \xi^j \right) -  f_0 {B^\prime}\indices{^q_k} B\indices{^q_j}\eta^j \right] \Delta t \\
\frac{\partial S}{\partial \eta^k} =& \sum_{q=1}^Q \left[ {B^\prime}\indices{^q_k} \left( {B^\prime}\indices{^q_j} \eta^j \right) -  f_0 {B^\prime}\indices{^q_j} \xi^j B\indices{^q_k} \right] \Delta t
\end{align}

Starting with $y$,
\begin{align}
0 =& \frac{\partial \phi}{\partial \eta^m} \\
=& - 2 y_k W\indices{^k_i}  B\indices{^i_m}+2 B\indices{_k_m} W\indices{^k_i}  B\indices{^i_l} \eta^l + \frac{2 \Delta t}{ \bar{v}^2 Q \Delta t} \left[ {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \eta^j -  f_0 {B^\prime}\indices{^q_j} B\indices{^q_m}  \xi^j \right] \\
y_k W\indices{^k_i}  B\indices{^i_m} =& \left[B\indices{_k_m} W\indices{^k_i}  B\indices{^i_j} + \frac{1}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \right] \eta^j - \frac{f_0}{ \bar{v}^2 Q} {B^\prime}\indices{^q_j} B\indices{^q_m}  \xi^j.
\end{align}
Without the Coriolis portion, this would just be
\begin{equation}
\eta^j = \left[B\indices{_k_m} W\indices{^k_i}  B\indices{^i_j} + \frac{1}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \right]^{-1} y_k W\indices{^k_i}  B\indices{^i_m}
\end{equation}
but with the Coriolis portion, the solution is now coupled to what is happening along the other axis.

And then with $x$,
\begin{align}
0 =& \frac{\partial \phi}{\partial \xi^m} \\
=&  - 2 x_k W\indices{^k_i}  B\indices{^i_m}+2 B\indices{_k_m} W\indices{^k_i}  B\indices{^i_l} \xi^l + \frac{2 \Delta t}{ \bar{v}^2 Q \Delta t} \left[ {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \xi^j  -  f_0 {B^\prime}\indices{^q_m} B\indices{^q_j}\eta^j \right]
\end{align}
which means that,
\begin{align}
x_k W\indices{^k_i}  B\indices{^i_m} =& \left[B\indices{_k_m} W\indices{^k_i}  B\indices{^i_j} + \frac{1}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \right] \xi^j - \frac{f_0}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m} B\indices{^q_j}  \eta^j.
\end{align}

So, we have the following two equations,
\begin{align}
x_k W\indices{^k_i}  B\indices{^i_m} =& \left[B\indices{_k_m} W\indices{^k_i}  B\indices{^i_j} + \frac{1}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \right] \xi^j - \frac{f_0}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m} B\indices{^q_j}  \eta^j \\
y_k W\indices{^k_i}  B\indices{^i_m} =&  - \frac{f_0}{ \bar{v}^2 Q} {B^\prime}\indices{^q_j} B\indices{^q_m}  \xi^j + \left[B\indices{_k_m} W\indices{^k_i}  B\indices{^i_j} + \frac{1}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \right] \eta^j
\end{align}
Which are simply solved when written in the form,
\begin{equation}
\left[\begin{array}{c} x_k W\indices{^k_i}  B\indices{^i_m} \\ y_k W\indices{^k_i}  B\indices{^i_m} \end{array}\right] = A \cdot \left[\begin{array}{c} \xi^j \\ \eta^j \end{array}\right]
\end{equation}

Adding in the Lagrange constraints that $F\indices{^i_j}\xi^j = h^i$ where the first index is the number of constraints and the second index is the number of parameters, I get that
\begin{align}
x_k W\indices{^k_i}  B\indices{^i_m} =& \underbrace{\left[B\indices{_k_m} W\indices{^k_i}  B\indices{^i_j} + \frac{1}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \right]}_a \xi^j \underbrace{- \frac{f_0}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m} B\indices{^q_j}}_b  \eta^j +  A\indices{^j_m} \lambda^j - f_0 X\indices{^j_m} \mu^j \\
y_k W\indices{^k_i}  B\indices{^i_m} =& \underbrace{ - \frac{f_0}{ \bar{v}^2 Q} {B^\prime}\indices{^q_j} B\indices{^q_m}}_c  \xi^j + \underbrace{\left[B\indices{_k_m} W\indices{^k_i}  B\indices{^i_j} + \frac{1}{ \bar{v}^2 Q} {B^\prime}\indices{^q_m}{B^\prime}\indices{^q_j} \right]}_d \eta^j + f_0 V\indices{^j_m} \lambda^j  + A\indices{^j_m} \mu^j \\
h^i =& A\indices{^i_j}\xi^j + f_0 V\indices{^i_j}\eta^j \\
h^i =& - f_0 X\indices{^i_j}\xi^j +A\indices{^i_j}\eta^j
\end{align}
Our vector is now,
\begin{equation}
\left[ \begin{array}{cccc}
a & b & F\indices{_j_m} & 0\\
c & d & 0 & F\indices{_j_m} \\
F\indices{^i_j} & 0 & 0 & 0 \\
0 & F\indices{^i_j} & 0 & 0 \end{array} \right]
 \cdot \left[\begin{array}{c} \xi^j \\ \eta^j \\ \lambda \\ \mu \end{array}\right] = \left[\begin{array}{c} x_k W\indices{^k_i}  B\indices{^i_m} \\ y_k W\indices{^k_i}  B\indices{^i_m} \\ h \\ h \end{array}\right]
\end{equation}

Trying to be careful about the endpoint conditions,
\begin{equation}
\phi = ...+ \lambda^i \left(  A\indices{^i_j}\xi^j + f_0 V\indices{^i_j}\eta^j - h^i \right)
\end{equation}
where $i$ is the index on the condition. So, after differentiating, you're going to have,
\begin{align}
=&...\lambda^i \left(  A\indices{^i_j} \right) \\
=&...\lambda^i f_0 V\indices{^i_j}
\end{align}

In the setup in Teanby, there's a knot point one spacing before and one spacing after the curve. So, if you have only two points observed, then you'd need a minimum of 4 knot points.

Using the simple inertial oscillation example is really insightful.

\subsection{e-fold parameters}

We are now trying to weight nearby measurements so that the fits use nearby points with decreased weighting depending on how far apart those points are.

We will use the terminology that the \emph{decorrelation time}, $T_d$, is when the weighting decreases to $0.01$, where we assume that a value of weight $1$ is coincident with the fit. We're going to use two different weighting functions, a Gaussian and an exponential.

The weighting matrix has rows and columns corresponding to each observations.  If we let $\tau=t_k-t_i$, then
\begin{equation}
W\indices{_k_i} = e^{ - \frac{(\tau\indices{_k_i})^2}{2 T_g^2} }
\end{equation}
where $T_g^2 = - \frac{T_d^2}{2 \ln \gamma}$ where we've set $\gamma = 0.01$.

For an exponential this would be,
\begin{equation}
W\indices{_k_i} = e^{ - \frac{\tau\indices{_k_i}}{T_e} }
\end{equation}
where $T_e = - \frac{T_d}{\ln \gamma}$.

\subsection{Integration}

Need to increase the resolution of the quadrature grid, or simply compute the integral analytically.

The summations in Teanby don't work at the boundaries because he's explicitly integrating, yet doesn't fully resolve the splines at the edges. Is that true?

An an important step in the method is the integration of several quantities. At the moment we're using a simple rectangle integration, but this appears to be fairly limiting. It's looks like its best to compute these quantities beforehand.

Note that the integration matrix is scaled by $dt \cdot dk$ where $dk$ is knot spacing.

It is possible to set conditions on jerk (derivative of acceleration) using the cubic spline, but one has to be careful. It's is a piecewise discontinuous function, so one needs to be careful to include the knot points in the nonzero part of the function, then things work okay.

\subsection{quintic spline}

A quintic B-spline finite-element method for solving the nonlinear Schršdinger equation

\subsection{smoothing kernel}

At what scale do we smooth the forcing? This depends on both the physics, and the sampling properties of the data. The coolest part is that we can use the velocity spectrum to estimate how often we should sample the data, given the measurement errors. For example, if the particle experiences a velocity of $u \sin(\omega t)$ then we obviously have to sample at the Nyquist frequency with period ($T$), and we'd darn well have measurement errors less than $u/T$.

\subsection{constraints as springs}

Given that the `force' of a Gaussian-error observation increases linearly with distance, the observations can be interpreted as springs (which have a linearly increasing force). The only difference is that these springs are, by default, delta functions in time (I haven't confirmed this). That would mean that we could extend the notation of the springs from being delta functions in time, to having a more broad band signature.

If your penalty function looks like this,
\begin{equation}
\phi = \sum_i \frac{ [x_i - x(t_i) ]^2}{2 \sigma_i^2} + \frac{1}{\bar{u}^2 T}  \int \dot{x}^2(t)  \, dt 
\end{equation}
then you can rewrite it like this,
\begin{equation}
\phi = \frac{1}{\bar{u}^2 T}  \int \left[ \dot{x}^2(t) + \sum_i \frac{\bar{u}^2 T}{2 \sigma_i^2} [x_i - x(t)]^2 \delta(t-t_i)  \right] \, dt
\end{equation}
This has a very clear interpretation as a physical system. The observations $(t_i, x_i)$ are simply delta function springs with spring constants $k=\bar{u}^2 T/\sigma_i^2$.

The spring constant is interesting. The dependence on $\sigma$ make sense---the more confident you are about the position of the particle (smaller $\sigma$), the stronger the spring will pull the particle in that direction. On the other hand, the dependence on \emph{action}, $\bar{u}^2 T$, is not so obvious. As the action increases in values, this increases the strength of the spring. Conversely, if the action is viewed as normalizing the kinetic energy, it's effect is to decrease the contribution of large kinetic energies in the integral, and therefore favor larger kinetic energies when minimizing.

After further consideration, I think that the right way to view this is like this,
\begin{equation}
\phi =  \int \left[\frac{\dot{x}^2(t)}{\bar{u}^2 T}   + \sum_i \frac{1}{2 \sigma_i^2} [x_i - x(t)]^2 \delta(t-t_i)  \right] \, dt.
\end{equation}
The spring therefore increases it's relative strength as the uncertainty in the particle's position decreases, and the kinetic energy is seen to be normalized by an \emph{rms} value, assuming $T$ varies with the length of the time series. If $T$ did not vary with the length of the time series, then kinetic energy would become increasingly penalized as the time series grows in length. This is, apparently, what happens in quantum mechanics because $\hbar$ stays fixed. Although in that case $S$ represents a phase, so maybe its not so straightforward.

Nope, this isn't right. For our purposes $\bar{u}^2 T$ is just the characteristic acceleration. This is easy to see if you write this as a dynamical system because that's what scales acceleration.

At the very least we might suppose that these springs are acting on the time intervals between observations, rather than simple as a delta function in time. This make sense because we have literally no other information about what's happening between observations.

To reproduce the ball rolling on an inclined plane case, we take the opposite extreme and suppose that the in the absence of additional observations, the forcing was constant in time between observations (rather than a delta function). This can be written in terms of Heaviside step functions, $H$.
\begin{equation}
\phi = \frac{1}{\bar{u}^2 T}  \int \left[ \dot{x}^2(t) + \sum_i \frac{\bar{u}^2 T}{\sigma_i^2} [x_i - x(t)]^2 \left( H(t-t_{i-1}) - H(t-t_{i+1}) \right) \right] \, dt
\end{equation}
Conceptually this lets the spring exist starting at the time of the previous observation and ending and the time of the next observation. That means, at any given time there will be two springs acting on the particle.

Generalizing this, we have some forcing function (perhaps a spring) acting for some finite amount of time,
\begin{equation}
\phi = \frac{1}{\bar{u}^2 T}  \int \left[ \dot{x}^2(t) + \sum_i f^i(t,x) W^i(t) \right] \, dt
\end{equation}
where $f(t,x)$ is the forcing function and $W$ is the weighting function.

How do we solve this? Here's the delta function case,
\begin{align}
\phi = & \sum_q \left[ \left( V\indices{^q_j} \xi^j \right)^2 + \frac{\bar{u}^2 T}{\sigma_i^2 } \sum_i \left[ x^i \iota^q - X\indices{^q_j} \xi^j \right]^2 \delta\indices{^q_{t_i}}  \right] \Delta t
\end{align}
where I've used $\iota^q$ to represent a vector of $1$s. Worth remembering here is that $x^i$, in this context, is just a scalar value---and that's why I've used $\iota^q$ to convert it into a vector. The $q$ index is the integration index. The delta function $\delta\indices{^q_{t_i}}$ should be interpreted as producing a $1$ when the index $q$ is equal to $t_i$, and zero otherwise. 

Generalizing this slightly,
\begin{align}
\phi = & \sum_q \left[ \left( V\indices{^q_j} \xi^j \right)^2 + \frac{\bar{u}^2 T}{\sigma_i^2} \sum_i \left[ x^i \iota^q - \iota^i X\indices{^q_j} \xi^j \right]^2 W\indices{^q_i}  \right] \Delta t
\end{align}
$W\indices{^q_i}$ now allows for different time distributions of the forcing. This is just as before, but now $W\indices{^q_i}$ is a $Q\times N$ matrix. We can reduce the standard least squares method if the columns of $W$ contains $1$s at the time of each observation.

\begin{align}
\bar{X}^2 \equiv&  \left[ x^i \iota^q - \iota^i X\indices{^q_j} \xi^j \right] \left[ x^i \iota^q - \iota^i X\indices{^q_l} \xi^l \right] \\
=& x^i x^i \iota^q - 2 x^i  X\indices{^q_j} \xi^j+ \iota^i \xi_j X\indices{_q^j} X\indices{^q_l} \xi^l.
\end{align}


Repeated indices imply summation, so we can write this as,
\begin{align}
\phi = & \frac{\Delta t}{\bar{u}^2 T} V\indices{_q^k} \xi_k V\indices{^q_j} \xi^j + \frac{\Delta t}{\sigma_i^2} \left[  x^i x^i \iota^q - 2 x^i  X\indices{^q_j} \xi^j+ \iota^i \xi_j X\indices{_q^j} X\indices{^q_l} \xi^l \right] W\indices{^q_i} 
\end{align}
which means that
\begin{align}
\frac{\partial \phi}{\partial \xi^m} = & \frac{\Delta t}{\bar{u}^2 T} V\indices{_q_m} V\indices{^q_j} \xi^j +  \frac{\Delta t}{\sigma_i^2}\left[  - 2 x^i  X\indices{^q_m}+ 2 \iota^i X\indices{_q_m} X\indices{^q_l} \xi^l \right] W\indices{^q_i} 
\end{align}

The minimizing with respect to the parameters,
\begin{align}
0 =& \frac{\partial \phi}{\partial \xi^m} \\
0=&   \frac{1}{\bar{u}^2 T} V\indices{_q_m} V\indices{^q_j} \xi^j + \frac{1}{\sigma_i^2} \left[  - 2 x^i  X\indices{^q_m}+ 2 \iota^i X\indices{_q_m} X\indices{^q_l} \xi^l \right] W\indices{^q_i}   \\
0=& \left[\frac{1}{\bar{u}^2 T} V\indices{_q_m} V\indices{^q_j} + \frac{2}{\sigma_i^2} \iota^i X\indices{_q_m} X\indices{^q_j} W\indices{^q_i} \right] \xi^j - \frac{2}{\sigma_i^2}  x^i W\indices{^q_i}  X\indices{^q_m} \\
\xi^j =& \left[\frac{1}{\bar{u}^2 T}V\indices{_q_m} V\indices{^q_j} + \frac{2}{\sigma_i^2}  \iota^i X\indices{_q_m} X\indices{^q_j} W\indices{^q_i} \right]^{-1} \frac{2}{\sigma_i^2}  x^i W\indices{^q_i}  X\indices{^q_m}
\end{align}

\subsection{Off diagonal terms}

\begin{align}
& (x_0 - x(t_0))W(x_1 - x(t_1))
\end{align}

\subsection{Alternative hypothesis}

There's an alternative way to interpret the observations as forcing. Specifically,
\begin{equation}
\phi =  \int \left[\frac{\dot{x}^2(t)}{\bar{u}^2 T}   + \sum_i \frac{1}{2 \sigma_i^2} [x_i - x(t_i)] [x_i - x(t)] \delta(t-t_i)  \right] \, dt.
\end{equation}
The difference now is that this looks more like constant forcing because the potential is $\sim x$, instead of $\sim x^2$. Of course, we've still applied the force over an infinitesimal impulse. This can then be converted to a more general form,
\begin{equation}
\phi =  \int \left[\frac{\dot{x}^2(t)}{2 S_0}   + \sum_i \frac{1}{2 \sigma_i^2} [x_i - x(t_i)] [x_i - x(t)] W(t-t_i)  \right] \, dt.
\end{equation}
where $W(t-t_i)$ is the forcing decorrelation function that must integrate to $1$ in order to preserver impulse (change in momentum). Is that right? I need to think about the condition on this.

Either way, here's the discretized version,
\begin{align}
\phi = & \sum_q \left[ \frac{1}{2 S_0} \left( V\indices{^q_j} \xi^j \right)^2 + \frac{1}{2 \sigma_i^2} \sum_i \left[ x^i \iota^q - \iota^q \Xi \indices{^i_j} \xi^j \right] \left[ x^i \iota^q - \iota^i X\indices{^q_j} \xi^j \right] W\indices{^q_i}  \right] \Delta t
\end{align}
where the only difference from before is that the index $X\indices{^q_j} \xi^j$ is now switched to $\Xi \indices{^i_j}\xi^j$, indicating we only apply this at the points of observation.

Using that repeat indices are summed (in this case $i$ and $q$), we have that
\begin{align}
\phi = &  \frac{\Delta t}{2 S_0} \left( V\indices{^q_j} \xi^j \right)^2 + \frac{\Delta t}{2 \sigma_i^2} \left[ x^i \iota^q - \iota^q \Xi\indices{^i_j} \xi^j \right] \left[ x^i \iota^q - \iota^i X\indices{^q_k} \xi^k \right] W\indices{^q_i}  \\
=&\frac{\Delta t}{2 S_0} V\indices{^q_j} \xi^j V\indices{^q_k} \xi^k + \frac{\Delta t}{2 \sigma_i^2} \left[ x^i x^i \iota^q - x^i X\indices{^q_j} \xi^j - x^i \iota^q \Xi \indices{^i_j} \xi^j + \Xi \indices{^i_j} \xi^j X\indices{^q_k} \xi^k \right] W\indices{^q_i} 
\end{align}

Minimizing with respect to the parameters,
\begin{align}
0 =& \frac{\partial \phi}{\partial \xi^m}  \\
0=&  \frac{\Delta t}{S_0} V\indices{_q_m} V\indices{^q_j} \xi^j + \frac{\Delta t}{2 \sigma_i^2} \left[ - x^i X\indices{^q_m} - x^i \iota^q \Xi \indices{^i_m} +  \Xi \indices{^i_m} X\indices{^q_j} \xi^j +  \Xi \indices{^i_j} X\indices{^q_m} \xi^j \right] W\indices{^q_i} 
\end{align}
and then solving for the parameters yields,
\begin{align}
\nonumber
0=&  \left[ \frac{2}{S_0} V\indices{_q_m} V\indices{^q_j} + \frac{ 1}{\sigma_i^2} \left(\Xi \indices{^i_m} X\indices{^q_j} +  \Xi \indices{^i_j} X\indices{^q_m} \right) W\indices{^q_i}  \right] \xi^j - \frac{1}{\sigma_i^2} \left[  x^i X\indices{^q_m} + x^i \iota^q \Xi \indices{^i_m} \right] W\indices{^q_i} \\
\xi^j =& \left[ \frac{2}{S_0} V\indices{_q_m} V\indices{^q_j} +\frac{1}{\sigma_i^2} \left(\Xi \indices{^i_m} X\indices{^q_j} +  \Xi \indices{^i_j} X\indices{^q_m} \right) W\indices{^q_i}  \right]^{-1}  \frac{1}{\sigma_i^2} \left[ x^i X\indices{^q_m} + x^i \iota^q \Xi \indices{^i_m} \right] W\indices{^q_i}
\end{align}

Quick little primer on the matrices. You can treat the first index as the rows and the second index as the columns. It really doesn't matter if they're upper or lower indices for us in this case. However, if you want to keep track of that, then make the first index upper, and the second index lower.

\section{Rod forcing}

Compare this to the least-squares method,
\begin{equation}
\mathcal{L} = \frac{\dot{x}^2(t)}{2 S_0}   + \frac{1}{2 \sigma_i^2} [x_i - x(t_i)] [x_i - x(t)] \delta(t-t_i) 
\end{equation}
which when used in the Euler-Lagrange equations gives,
\begin{equation}
\ddot{x}(t) = \frac{S_0}{2\sigma_i^2} (x_i - x(t_i)) \delta(t-t_i)
\end{equation}
or,
\begin{equation}
\dot{x}(t) = v(0) + \frac{S_0}{2 \sigma_i^2} (x_i - x(t_i)) H\left(t - t_i \right).
\end{equation}
The recursive nature of this makes no sense at all.

If $S_0=0$, then we just have that there's no acceleration. This is the rigid rod  analogy. We then also have to add that the rod isn't rotating, e.g., there's no torque on it in order to get the second condition.


\section{True forcing}

The assumed forcing on the particle due to the observations can be deduced from any three neighboring points. This is because you need two points to deduce a velocity, and two velocities to deduce an acceleration and force.

\begin{equation}
\mathcal{L} = \frac{1}{2} \dot{x}^2 - \Delta v(t_i) \left( x(t) - x_i \right) \delta\left(t - t_i \right)
\end{equation}
where the velocity $v(t_i)$ is determined from the the points of observation, e.g.,
\begin{equation}
\mathcal{L} = \frac{1}{2} \dot{x}^2 - \left( \frac{x(t_{i+1}) - x(t_{i})}{t_{i+1} - t_{i}} - \frac{x(t_{i}) - x(t_{i-1})}{t_{i} - t_{i-1}} \right) \left( x(t) - x_i \right) \delta\left(t - t_i \right).
\end{equation}
Using the Euler-Lagrange equations, this gives us,
\begin{equation}
\ddot{x}(t) = \Delta v(t_i) \delta\left(t - t_i \right)
\end{equation}
which when integrated over time is just
\begin{equation}
\dot{x}(t) = v(0) + \Delta v(t_i) H\left(t - t_i \right).
\end{equation}
This is all fairly tautological in that we're just saying that the velocity is governed how the velocity changes.

Suppose the impulse is spread over some time $\Delta t$. Then
\begin{equation}
\ddot{x}(t) = \frac{\Delta v(t_i)}{\Delta t} \left( H(t- t_i + \Delta t/2) - H(t-t_i-\Delta t/2) \right)
\end{equation}
which integrates to,
\begin{equation}
\dot{x}(t) = v(0) + \frac{\Delta v(t_i)}{\Delta t} (t - t_i + \Delta t/2)  \left( H(t- t_i + \Delta t/2) - H(t-t_i-\Delta t/2) \right).
\end{equation}
This is nice because the velocity now changes linearly over the given time period.

This simplifies to an interesting, if not obvious, result if you take the time difference to be constant. In that case you get that,
\begin{equation}
\mathcal{L} = \frac{1}{2} \dot{x}^2 - \frac{1}{\Delta t} \left( x(t_{i+1}) - 2 x(t_{i}) + x(t_{i-1}) \right) \left( x(t) - x_i \right) \delta\left(t - t_i \right).
\end{equation}
In other words, the second derivative of positions gives you the forcing. Duh.

In terms of a weighting matrix, this can all be re-written as,
\begin{equation}
\mathcal{L} = \frac{1}{2} \dot{x}^2 - \frac{\Delta t_{+} + \Delta t_-}{\Delta t_+ \Delta t_-} \left[ \frac{\Delta t_-}{\Delta t_+ + \Delta t_-} x(t_{i+1}) - x(t_i) +\frac{\Delta t_+}{\Delta t_+ + \Delta t_-} x(t_{i-1}) \right] \left( x(t) - x_i \right) \delta\left(t - t_i \right)
\end{equation}
or 
\begin{equation}
\nonumber
\mathcal{L} = \frac{1}{2} \dot{x}^2 - \frac{\Delta t_{+} + \Delta t_-}{\Delta t_+ \Delta t_-} \left[ \left( 1 - \frac{\Delta t_+}{\Delta t_+ + \Delta t_-} \right) x(t_{i+1}) - x(t_i) + \left( 1 - \frac{\Delta t_-}{\Delta t_+ + \Delta t_-} \right) x(t_{i-1}) \right] \left( x(t) - x_i \right) \delta\left(t - t_i \right)
\end{equation}
This is very similar to our usual-off diagonal weighting, with point nearby in time getting higher weights than those further away. However, the sign of those nearby points is different than what we've been using.

Compare this to the least-squares method,
\begin{equation}
\mathcal{L} = \frac{\dot{x}^2(t)}{2 S_0}   + \frac{1}{2 \sigma_i^2} [x_i - x(t_i)] [x_i - x(t)] \delta(t-t_i) 
\end{equation}
which, even for correlated nearby points doesn't quite look right,
\begin{equation}
\mathcal{L} = \frac{\dot{x}^2(t)}{2 S_0}   + \frac{1}{2 \sigma_i^2} \left(w_{i+1} [x_{i+1} - x(t_{i+1})]  + [x_i - x(t_i)]  + w_{i-1} [x_{i-1} - x(t_{i-1})] \right) [x_i - x(t)] \delta(t-t_i) 
\end{equation}
This weighting would pull the fit closer to the neighboring points, if the current point is spurious. So the weighting should be positive, this makes sense. We could set the weighting to be the second derivative, but then we end up with something that looks like the second derivative of the fit minus the second derivative of the observations. I don't think that's what we want.

So, our uncertainty is on the positions, that's true. But, the positions are telling us something about the forcing and we know (or think we know) how the the forcing should be correlated in time. We have some function,
\begin{equation}
f_i \Sigma_{ij}^{-1} f_j
\end{equation}

What we want, is to decrease the force at the current point, if the current forcing isn't correlated with past forcing.

So what if you just straight add a least squares method and the known forcing bit,

\begin{equation}
\mathcal{L} = \frac{\dot{x}^2(t)}{2 S_0}   + \frac{1}{2 \sigma_i^2} \left( - \frac{2 \sigma_i^2}{S_0} \Delta v(t_i) + x_i - x(t_i)\right) [x_i - x(t)] \delta(t-t_i) 
\end{equation}

Note that this suggests an alternative definition for $S_0$ of $\sigma^2/\Delta t$. This is probably a good choice because it's an assessment of what kind of curvature you'd expect from your noise.

\section{Constant forcing}

\begin{equation}
\phi = \frac{\Delta t}{S_0} \left[ \frac{1}{2} \left( V\indices{^q_j} \xi^j \right)^2  - \left( \mathcal{D}\indices{^i_j} \Xi\indices{^j_k}\xi^k \right) \left( X\indices{^q_l} \xi^l W\indices{^q_i} \right)   \right] + \frac{1}{2} \left[ x^k - \Xi\indices{^k_j} \xi^j \right]^{\textrm{T}} {\Sigma^{-1}}\indices{^k_i} \left[ x^i - \Xi\indices{^i_l} \xi^l \right] 
\end{equation}

The expression $ \mathcal{D}\indices{^i_j} \Xi\indices{^j_k}\xi^k$ takes the difference of the first derivative of the path at the observation points. This could be renamed $f^i$, the force at each observation time $i$. The next component is $X\indices{^q_j} \xi^j W\indices{^q_i}$, which is just the path at all points, but weighted differently for each observation $i$. In fact, we're starting with delta function weighting, so that each observed forcing only occurs at that moment.

If I've got this correct,
\begin{align}
\nonumber
\frac{\partial \phi}{\partial \xi^m} =& \frac{\Delta t}{S_0} \left[ V\indices{^q_m} V\indices{^q_j} \xi^j - \left( \mathcal{D}\indices{^i_j} \Xi\indices{^j_m} X\indices{^q_l} \xi^l W\indices{^q_i} +  \mathcal{D}\indices{^i_j} \Xi\indices{^j_k}\xi^k X\indices{^q_m} W\indices{^q_i} \right) \right] \\
& - 2 x_k {\Sigma^{-1}}\indices{^k_i}  X\indices{^i_m}+2 X\indices{_k_m} {\Sigma^{-1}}\indices{^k_i}  X\indices{^i_l} \xi^l \\ \nonumber
=&  \frac{\Delta t}{S_0} \left[ V\indices{^q_m} V\indices{^q_n} - \left( \mathcal{D}\indices{^i_j} \Xi\indices{^j_m} X\indices{^q_n} W\indices{^q_i} +  \mathcal{D}\indices{^i_j} \Xi\indices{^j_n} X\indices{^q_m} W\indices{^q_i} \right) + \frac{2S_0}{\Delta t} X\indices{_k_m} {\Sigma^{-1}}\indices{^k_i}  X\indices{^i_n} \right] \xi^n\\
& - 2 x_k {\Sigma^{-1}}\indices{^k_i}  X\indices{^i_m}
\end{align}
which means that we want to find $\xi$ where,
\begin{align}
2 x_k {\Sigma^{-1}}\indices{^k_i}  X\indices{^i_m} =& \frac{\Delta t}{S_0} \left[ V\indices{^q_m} V\indices{^q_n} - \left( \mathcal{D}\indices{^i_j} \Xi\indices{^j_m} X\indices{^q_n} W\indices{^q_i} +  \mathcal{D}\indices{^i_j} \Xi\indices{^j_n} X\indices{^q_m} W\indices{^q_i} \right) + \frac{2S_0}{\Delta t} X\indices{_k_m} {\Sigma^{-1}}\indices{^k_i}  X\indices{^i_n} \right] \xi^n
\end{align}

\section{Fitting to velocity}

Let's define
\begin{equation}
x_i = x(t_i) + \epsilon_i
\end{equation}
such that the observed value $x_i$ is the true value $x(t_i)$ plus some noise, $\epsilon_i$. Our noise may be normally distributed such that,
\begin{equation}
P(\epsilon_i) = \frac{1}{\sigma_i \sqrt{2 \pi}} e^{- \frac{\epsilon_i^2}{2 \sigma_i^2}}.
\end{equation}
Now if we compute the difference,
\begin{equation}
x_{i+1}-x_{i} = x(t_{i+1}) - x(t_i) +  \epsilon_{i+1} - \epsilon_i
\end{equation}
Then the errors in the difference are just the convolution of the two Gaussians, which is again a Gaussian. Meaning that the errors will look like
\begin{equation}
P(\epsilon_{i+1}- \epsilon_i) = \frac{1}{ \sqrt{2 \pi (\sigma_{i+1}^2+\sigma_i^2) }} e^{- \frac{(\epsilon_{i+1} - \epsilon_i)^2}{2 \left(\sigma_{i+1}^2+\sigma_i^2\right)}}.
\end{equation}
In terms of velocity, this is
\[
v_i = v(t_i) + \delta_i
\]
\begin{equation}
P(\epsilon_{i+1}- \epsilon_i) = \frac{1}{ \sqrt{2 \pi (\sigma_{i+1}^2+\sigma_i^2) }} e^{- \frac{\Delta t^2 \left(v_i - v(t_i)\right)^2}{2 \left(\sigma_{i+1}^2+\sigma_i^2\right)}}.
\end{equation}

It's easy to find the variance of the error in position difference, 
\begin{align}
\textrm{var}(\epsilon_{i+1} - \epsilon_i) =& \int_{-\infty}^{\infty} \frac{ \left(\epsilon_{i+1} - \epsilon_i\right)^2}{ \sqrt{2 \pi (\sigma_{i+1}^2+\sigma_i^2) }} e^{- \frac{(\epsilon_{i+1} - \epsilon_i)^2}{2 \left(\sigma_{i+1}^2+\sigma_i^2\right)}} d \left(\epsilon_{i+1} - \epsilon_i\right) \\
=& \sigma_{i+1}^2+\sigma_i^2
\end{align}
and it's even easier if we note that $\textrm{cov}(\epsilon_{i+1}, \epsilon_{i+1} )=\sigma_{i+1}^2$, while $\textrm{cov}(\epsilon_{i+1}, \epsilon_{i} ) = 0$.

This means that the covariance is,
\begin{align}
\textrm{cov}(\epsilon_{i+2} - \epsilon_{i+1},\epsilon_{i+1} - \epsilon_i) =& \textrm{cov}(\epsilon_{i+2},\epsilon_{i+1} - \epsilon_i) - \textrm{cov}( \epsilon_{i+1},\epsilon_{i+1} - \epsilon_i) \\
=& -\sigma_{i+1}^2
\end{align}
for neighboring elements, and zero otherwise. The covariance matrix thus has the form,
\begin{equation}
\Sigma = 
\left[\begin{array}{ccccc}\sigma_2^2+\sigma_1^2 & -\sigma_2^2 & 0 & 0 & 0 \\-\sigma_2^2 & \sigma_3^2 +\sigma_2^2 & -\sigma_3^2 & 0 & 0 \\0 & -\sigma_3^2 &  &  & 0 \\0 & 0 &  & \sigma_{i+1}^2 + \sigma_{i}^2 & -\sigma_{i+1}^2 \\0 & 0 & 0 & -\sigma_{i+1}^2 & \end{array}\right]
\end{equation}
Let's fix some notation. What if we define $\epsilon^x_i$ as the random variable describing the error in position ($x$) for the $i$-th observation. We can then let $\epsilon^{dx}_i \equiv \epsilon^x_{i+1} - \epsilon^x_i$ describe the error in the $i$-th observation of velocity.

The covariance in position is simply,
\begin{align}
\textrm{cov}(\epsilon^x_i, \epsilon^x_j) = \sigma_i^2 \delta_{ij}
\end{align}
but the covariance in position difference is,
\begin{align}
\textrm{cov}(\epsilon^{dx}_i, \epsilon^{dx}_i) =& \sigma_{i+1}^2 + \sigma_i^2 \\
\textrm{cov}(\epsilon^{dx}_{i+1}, \epsilon^{dx}_i) =& -\sigma_{i+1}^2 \\
\textrm{cov}(\epsilon^{dx}_{i}, \epsilon^{dx}_{i+1}) =& -\sigma_{i+1}^2
\end{align}
which can be written in matrix form as,
\begin{equation}
\Sigma_{ij} = (\sigma_{i+1}^2 + \sigma_i^2) \delta_{ij} - \sigma_{i}^2 \delta_{i-1,j} - \sigma_{i+1}^2 \delta_{i,j-1}.
\end{equation}
Now let's define the random variable describing the error in velocity as $\epsilon^v_i \equiv \frac{\epsilon^{dx}_i}{\Delta t_i}$ where $\Delta t_i = t_{i+1} - t_i$. Using the property that $\textrm{cov}(cX,Y)=c\,\textrm{cov}(X,Y)$, 
\begin{align}
\textrm{cov}(\epsilon^{v}_i, \epsilon^{v}_i) =& \frac{1}{\Delta t_i^2} \left( \sigma_{i+1}^2 + \sigma_i^2 \right) \\
\textrm{cov}(\epsilon^{v}_{i+1}, \epsilon^{v}_i) =& -\frac{1}{\Delta t_{i+1} \Delta t_i}\sigma_{i+1}^2 \\
\textrm{cov}(\epsilon^{v}_{i}, \epsilon^{v}_{i+1}) =& -\frac{1}{\Delta t_{i+1} \Delta t_i}\sigma_{i+1}^2
\end{align}
This is exactly the covariance matrix we need to to solve the linear least squares problem. So far we have not taken into account boundary conditions, or higher order matrices.

\subsection{Second derivative}

What we really want is the forcing... and we want it for unevenly spaced data. 

Starting simple, let's define $\epsilon^{dv}_i \equiv \epsilon^{dx}_{i+1} - \epsilon^{dx}_i$ which means that $\epsilon^{dv}_i = \epsilon_{i+2}  - 2 \epsilon_{i+1} + \epsilon_i$. The covariance matrix can now be constructed from the following,
\begin{align}
\textrm{cov}(\epsilon^{dv}_i, \epsilon^{dv}_i) =&\sigma_{i+2}^2 + 4 \sigma_{i+1}^2 + \sigma_i^2 \\
\textrm{cov}(\epsilon^{dv}_{i+1}, \epsilon^{dv}_i) =& -2 \sigma_{i+2}^2 -2 \sigma_{i+1}^2 \\
\textrm{cov}(\epsilon^{dv}_{i+2}, \epsilon^{dv}_{i}) =& \sigma_{i+2}^2
\end{align}
and their symmetric counterparts.

We're now in a position to define the random variable describing the error in the acceleration as $\epsilon^{a}_i \equiv \frac{2}{ \Delta t_{i+1} + \Delta t_i } \left( \epsilon^{v}_{i+1} - \epsilon^{v}_i\right)$. This definition is valid to second order accuracy for unevenly spaced data. We can rewrite this as,
\[
\epsilon^{a}_i \equiv \frac{2 \epsilon_{i+2} }{ (\Delta t_{i+1} + \Delta t_i) \Delta t_{i+1} } - \frac{2 \epsilon_{i+1} }{ \Delta t_{i+1}  \Delta t_i } + \frac{2 \epsilon_{i} }{ (\Delta t_{i+1} + \Delta t_i) \Delta t_{i} }
\]
The second-order second-derivative at the boundary is somewhat different,
\begin{multline}
\epsilon^a_0 = 2 \epsilon_0\frac{ \left( 3\Delta t_0 + 2 \Delta t_1 + \Delta t_2 \right)}{\Delta t_0 (\Delta t_0 + \Delta t_1) (\Delta t_0 + \Delta t_1 + \Delta t_2)} - 2 \epsilon_1 \frac{ (2\Delta t_0+ 2\Delta t_1+\Delta t_2)}{(\Delta t_1+\Delta t_2)\Delta t_0\Delta t_1} \\
 + 2 \epsilon_2 \frac{ 2 \Delta t_0 + \Delta t_1 + \Delta t_2}{\Delta t_1 \Delta t_2 (\Delta t_0 + \Delta t_1)} - 2 \epsilon_3 \frac{ 2 \Delta t_0 + \Delta t_1}{ \Delta t_2 (\Delta t_1 + \Delta t_2)(\Delta t_0+\Delta t_1+\Delta t_2)}.
\end{multline}

\subsection{Second derivative covariance matrix}
We can now construct the covariance matrix using the above two expressions. The interior of the matrix is constructed as follows,
\begin{align}
\textrm{cov}(\epsilon^{a}_i, \epsilon^{a}_i) =&\frac{ 4 \sigma_{i+2}^2}{(\Delta t_{i+1} + \Delta t_i)^2 \Delta t_{i+1}^2} + \frac{4 \sigma_{i+1}^2}{ \Delta t_{i+1}^2  \Delta t_i^2} + \frac{4\sigma_i^2}{(\Delta t_{i+1} + \Delta t_i)^2 \Delta t_{i}^2 } \\
\textrm{cov}(\epsilon^{a}_{i+1}, \epsilon^{a}_i) =& - \frac{4 \sigma_{i+2}^2}{ (\Delta t_{i+1} + \Delta t_i) \Delta t_{i+2} {\Delta t_{i+1}}^2 } - \frac{4 \sigma_{i+1}^2}{(\Delta t_{i+2} + \Delta t_{i+1}) {\Delta t_{i+1}}^2 \Delta t_i } \\
\textrm{cov}(\epsilon^{a}_{i+2}, \epsilon^{a}_{i}) =& \frac{ 4 \sigma_{i+2}^2}{(\Delta t_{i+1} + \Delta t_i) \Delta t_{i+1} (\Delta t_{i+2} + \Delta t_{i+1}) \Delta t_{i+2}}.
\end{align}

In a practical sense, all we have to do is create the finite difference matrix, then multiply each row by each other row.
\[
\Sigma\indices{_i_j} = \left[\mathcal{D}^2\right]\indices{_i_k}\left[\mathcal{D}^2\right]\indices{_j_k} \sigma_k \sigma_k
\]
I have no proof for this, but I believe we are stuck with two less data points than the position data. We only have $N-2$ independent observations of forcing. In a practical sense, this means we have no information about what's happening at the boundaries. This is evident in the $\mathcal{D}^2$ matrix which has a vanishing determinant if you include boundary condition terms, even at second order.

\subsection{Penalty function}
So we're going to have a penalty function like this,
\begin{equation}
\phi =  \frac{1}{2} \left[ \left[\mathcal{D}^2\right]\indices{^j_k} x^k - A\indices{^k_j} \xi^j \right]^{\textrm{T}} {\Sigma^{-1}}\indices{^k_i} \left[ \left[\mathcal{D}^2\right]\indices{^l_i} x^i - A\indices{^i_l} \xi^l \right] 
\end{equation}
which is just a best fit line through the forcing. So how to I enforce that we pass through (or near) the endpoints? Easy, we need need to also include the locations in the penalty function. So,
\begin{multline}
\phi =  \frac{1}{2} \left[ \left[\mathcal{D}^2\right]\indices{^j_k} x^k - A\indices{^k_j} \xi^j \right]^{\textrm{T}} {\Sigma_A^{-1}}\indices{^k_i} \left[ \left[\mathcal{D}^2\right]\indices{^l_i} x^i - A\indices{^i_l} \xi^l \right] \\
+  \left[  x^k - X\indices{^k_j} \xi^j \right]^{\textrm{T}} {\Sigma_{X}^{-1}}\indices{^k_i} \left[  x^i - A\indices{^i_l} \xi^l \right]
\end{multline}
Recall that,
\begin{align}
\bar{X}^2 \equiv&  \left[ x^k - X\indices{^k_j} \xi^j \right]^{\textrm{T}} {\Sigma_{X}^{-1}}\indices{^k_i} \left[ x^i - X\indices{^i_l} \xi^l \right] \\
=& x_k {\Sigma_{X}^{-1}}\indices{^k_i} x^i - 2 x_k {\Sigma_{X}^{-1}}\indices{^k_i}  X\indices{^i_j} \xi^j+ \xi_j X\indices{_k^j} {\Sigma_{X}^{-1}}\indices{^k_i}  X\indices{^i_l} \xi^l.
\end{align}
Which means that the extremum can be found with,
\begin{align}
0 =& \frac{\partial \phi}{\partial \xi^m} \\ \nonumber
=&  -  x_k {\Sigma_{X}^{-1}}\indices{^k_i}  X\indices{^i_m}+ X\indices{_k_m} {\Sigma_{X}^{-1}}\indices{^k_i}  X\indices{^i_l} \xi^l + \frac{ \Delta t}{ S_0 } \left[ {V^\prime}\indices{^q_m}{V^\prime}\indices{^q_j} \xi^j \right] \\
& -  \left[\mathcal{D}^2\right]\indices{^j_k} x_j {\Sigma_{A}^{-1}}\indices{^k_i}  A\indices{^i_m}+ A\indices{_k_m} {\Sigma_{A}^{-1}}\indices{^k_i}  A\indices{^i_l} \xi^l.
\end{align}
We can solve for $\xi$,
\begin{equation}
x_k {\Sigma_{X}^{-1}}\indices{^k_i}  X\indices{^i_m} + \left[\mathcal{D}^2\right]\indices{^j_k} x_j {\Sigma_{A}^{-1}}\indices{^k_i}  A\indices{^i_m} = \left[X\indices{_k_m} {\Sigma_{X}^{-1}}\indices{^k_i}  X\indices{^i_j} + A\indices{_k_m} {\Sigma_{A}^{-1}}\indices{^k_i}  A\indices{^i_j} + \frac{ \Delta t}{ S_0 }  {V^\prime}\indices{^q_m}{V^\prime}\indices{^q_j} \right] \xi^j
\end{equation}

\section{Notes}
\begin{enumerate}
\item For S=3, my second derivative constraint appears to be cutoff? Or, actually, it's probably just not a valid constraint for S=3. Need S=5.
\item Setting the jerk=0 at the end points doesn't seem to have the intended effect. Do I need to specify the forcing (as by observation) at the end points? Simply copying the previous forcing point over doesn't work because the covariance matrix is not invertible. Perhaps I need to go to higher order to make that work. Not sure.
\item Okay, so over constraining the problem does lead to the intended results for acceleration regardless of boundary conditions. That said, setting jerk=0 at the end points does change the endpoint behavior for jerk. However, the is still some squirrelly business near the end points. This means we probably do want a constraint on forcing near the end.
\item It is best to leave the x and y errors uncorrelated (as in, don't let the iteratively reweighted least-squares method learn about the errors in the orthogonal direction). The reason is that the errors are expected to be anisotropic. Think about dilution of precision.
\item It's important to choose a good initial value for $\sigma$. For a Gaussian fit this doesn't matter, but when using the IRLS method, this value is what determines whether or not points start to get rejected or not. This may want to be an rms value? I'm not sure. For my GPS data it looks like 10 meters may be too small, and 50 meters may be too big.
\item A SplineFactor (number of data points for each spline) of 1.5 does not appear to be enough to provide appropriate smoothness. A value of 3 starts to look borderline too smooth, but maybe not. Note that this is evaluated without implemented the ocean Lagrangian.
\item Look at the form that tension takes relative to the other terms creating that matrix. It's clear that the tension can be thought of as a diagonal $\Sigma^{-1}$ matrix with $\Delta t/S_0$ along the diagonal.
\item Setting my spline factor (e.g., number of observation points per spline node) has an interesting effect on the errors. You start to see stuff pulled away from the mean and create what looks like a mini-gaussian. Doesn't seem right.
\end{enumerate}

\section{Ocean forcing}

Now we need to return to fixing up our forcing constraint appropriate for ocean physics. This means that the forcing in the $x$-direction is $f_x(t) = \ddot{x} - f_0 \dot{y}$ and the forcing in the $y$ direction is $f_y(t) = \ddot{y} + f_0 \dot{x}$. So the question here is what do the errors of this measurement look like and how to we want to state that? This is probably just as simple as adding them.

\section{Derivative compatibility conditions}

Now I'm questioning which derivatives we attempt to match up and why. I mean, why don't we use the velocity, instead of just the acceleration. How about the jerk? What about even higher derivatives? I think, intuitively, the answer is that those aren't providing more information that will help to constrain our splines. But why?

The answer probably has to do the the data density relative to the data error. I think this is a one-dimension continuum where at one end, you have sparse observations, with relatively good positioning and at the other end you have dense observations with big errors. Specifically, this means that maybe the expected distance traversed ($u_{\textrm{rms}}\Delta t$) between observation times ($\Delta t$) is greater than the expected error $\sigma_x$. This means that you may want to interpolate between observations. At the other extreme, the expected distance traverse between observations is much smaller than the error.

Let's define the interpolation condition as,
\begin{equation}
\Gamma_x = \frac{\sigma_x}{u_{\textrm{rms}}\Delta t}
\end{equation}
So that if $\Gamma<1$ then we are in the interpolation regime, and if $\Gamma >1$ we are in the high data density regime where I don't suppose we'll need to interpolate. A value of $\Gamma = 5$ means that we have roughly $5$ observations within the region of each `point'. 

The variance for the first derivative is approximately $\sqrt{2}\sigma_x/\Delta t$ and we can therefore define a new $\Gamma$,
\begin{equation}
\Gamma_u = \frac{\sqrt{2} \sigma_x}{a_{\textrm{rms}}{\Delta t}^2}
\end{equation}
which reflects how much the velocity typical changes between observations.

We can keep going to higher and higher derivatives, and what should happen, is that we get,
\begin{equation}
\Gamma_{x^{(n-1)}} = \frac{ \sqrt{n!} \sigma_x}{x^{(n)}_{\textrm{rms}} (\Delta t)^n }
\end{equation}
where $x^{(n)}$ is the $n$-th derivative of $x$. Eventually $\Gamma$ will increase above 1?

I think we can also look at this just by saying that if we keep taking derivatives of the data, eventually it'll look like white noise.

\section{Spline types}

There are apparently smoothing splines and interpolating splines. Interpolating splines you start matching the multiple derivatives of the function in question, as I have done. Smoothing splines you start by adding a tension parameter. Actually, it's an acceleration parameter that you limit.

In the smoothing spline the add a tension as a way to control the derivatives of the spline. With our approach, we may not need this. We will try to control the derivatives of the spline simply be taking derivatives of the data. The point at which the derivatives become pure noise, is the point at which the derivatives of the splines are set to zero. This should all be the same in the end.

\section{t-distribution}

The pdf of the $t$-distribution is
\begin{equation}
p_t\left(x|\nu,\sigma^2\right) = \frac{\Gamma\left( \frac{\nu + 1}{2} \right)}{\sigma \sqrt{\nu \pi} \Gamma\left(\frac{\nu}{2}\right)} \left( 1 + \frac{x^2}{\sigma^2 \nu} \right)^{-\frac{\nu+1}{2}}
\end{equation}
where the $\sigma$ parameter scales the distribution width and the $\nu$ parameter sets the number of degrees of freedom. The variance is $\textrm{var}(X)=\sigma^2 \frac{\nu}{\nu-2}$ and only exists for $\nu > 2$.

For $\nu=3$ this becomes,
\begin{equation}
p_t\left(x|3,\sigma^2\right) = \frac{2}{\pi \sigma \sqrt{3} } \left( 1 + \frac{x^2}{3 \sigma^2} \right)^{-2}
\end{equation}
with variance $3 \sigma^2$.

Compare this to a Gaussian distribution,
\begin{equation}
p_g\left(x|\sigma^2\right) = \frac{1}{ \sigma \sqrt{2 \pi}} e^{-\frac{x^2}{2 \sigma^2}}
\end{equation}
with variance $\sigma^2$.

\section{Iteratively Reweighted Least Squares}

Apparently I independently discovered iteratively re-weighted least squares. The idea is this.

Following Press, et al., we start with the maximum likelihood using a Gaussian distribution,
\begin{equation}
\label{maxlikelihood}
P = \prod_{i=1}^N \frac{e^{-\rho} }{\sigma \sqrt{ 2 \pi}} \Delta y
\end{equation}
where $\rho_g = \frac{1}{2} \left( \frac{x_i - x(t_i,\mathbf{a})}{\sigma_i} \right)^2$ where $(t_i,x_i)$ is the data and $x(t,\mathbf{a})$ is the model with parameters $\mathbf{a}$. We then try to find the model parameters which maximize the likelihood, or equivalently we try to minimize,
\begin{equation}
\sum_{i=1}^N \rho \left( x_i - x(t_i,\mathbf{a}) \right).
\end{equation}
with respect to $\mathbf{a}$.

Continuing to (mostly) follow Press, et al., now let's define $\epsilon_i \equiv  x_i - x(t_i,\mathbf{a})$ and  $\psi \equiv \frac{d \rho}{d\epsilon}$. This means that our minimization condition becomes,
\begin{equation}
0 = \sum_{i=1}^N \psi(\epsilon_i) \frac{\partial x(t_i,\mathbf{a})}{\partial \mathbf{a}}.
\end{equation}
The Gaussian distribution looks like $\rho_g(z) = \frac{1}{2} \frac{z^2}{\sigma^2}$ and therefore $\psi_g(z)=\frac{z}{\sigma^2}$.

Using a Gaussian distribution is equivalent to solving the least-squares problem,
\begin{equation}
\label{gaussian_likelihood}
0 = \sum_{i=1}^N \frac{\epsilon_i}{\sigma_i^2} \frac{\partial x(t_i,\mathbf{a})}{\partial \mathbf{a}},
\end{equation}
a problem that we can solve linearly.

Using a $t$-distribution is equivalent to maximizing equation \ref{maxlikelihood} where now,
\begin{equation}
\rho_t(\epsilon_i|\nu) = - \log \left( \frac{\sigma_g \sqrt{2} \Gamma\left( \frac{\nu + 1}{2} \right)}{\sigma \sqrt{\nu} \Gamma\left(\frac{\nu}{2}\right)} \left( 1 + \frac{\epsilon_i^2}{ \nu \sigma^2} \right)^{-\frac{\nu+1}{2}} \right)
\end{equation}
\begin{equation}
\rho_t(\epsilon_i|3) = - \log \left( \frac{2 \sqrt{2}}{\sqrt{3 \pi}} \left( 1 + \frac{\epsilon_i^2}{3 \sigma_i^2} \right)^{-2} \right)
\end{equation}
and 
\begin{equation}
\psi_t(\epsilon_i) = \frac{\epsilon_i}{\sigma^2} \frac{\nu+1}{\nu} \left( 1 + \frac{\epsilon_i^2}{\nu \sigma^2} \right)^{-1}
\end{equation}
\[
\psi_t(\epsilon_i|3) = \frac{ 4 \epsilon_i}{3 \sigma_i^2} \left( 1 + \frac{\epsilon_i^2}{3 \sigma_i^2} \right)^{-1}.
\]
However, the equation 
\begin{equation}
0 = \sum_{i=1}^N \psi_t(\epsilon_i) \frac{\partial x(t_i,\mathbf{a})}{\partial \mathbf{a}}
\end{equation}
is not linear, so we cannot easily find the coefficients using this method. The trick is to re-weight the $\sigma_i$ in equation \ref{gaussian_likelihood} to make the errors look like $t$-distribution. That is, we want,
\begin{equation}
\frac{\epsilon_i}{\sigma_i^2} = \psi_t(\epsilon_i)
\end{equation}
or
\begin{equation}
\sigma_i^2 = \frac{\epsilon_i}{ \psi_t(\epsilon_i)}.
\end{equation}

It's easy to to consider what would happen if you simply used a Gaussian distribution again, but with a different $\sigma$. In that case, $\sigma_i^2 = \sigma_i^2$ where I need to use better notation. But see, it works.

You could avoid the divide by zero issue by precomputing this. So let's define,
\begin{align}
w_t(\epsilon_i) \equiv& \frac{\epsilon_i}{ \psi_t(\epsilon_i)}\\
=& \frac{3}{4} \sigma_i^2  \left( 1 + \frac{\epsilon_i^2}{3 \sigma_i^2} \right)
\end{align}

Or in general
\begin{equation}
w(\epsilon_i) = \sigma^2 \frac{\nu}{\nu+1} \left( 1 + \frac{\epsilon_i^2}{\nu \sigma^2} \right)
\end{equation}

\section{Refined penalty function}
The previous penalty function wasn't quite right. In fact, we should have
So we're going to have a penalty function like this,
\begin{equation}
\phi =  \frac{1}{2} \left[ \left[\mathcal{D}^2\right]\indices{^j_k} x^k - I\indices{^k_q}A\indices{^q_j} \xi^j \right]^{\textrm{T}} {\Sigma^{-1}}\indices{^k_i} \left[ \left[\mathcal{D}^2\right]\indices{^l_i} x^i - I\indices{^k_q}A\indices{^q_j} \xi^l \right] 
\end{equation}
where $I\indices{^k_q}$ is the integration function and the index $q$ is the integration grid. The point here being that instead of requiring the velocity or acceleration to agree at a particular point, we are requiring the average velocity or acceleration to agree over the interval between appropriate observations.

What are the consequences of this change? For one, this would allow delta-function forcing because the integral would provide the same change in momentum. This take me back to square one. It's not helpful at all and proves that everything I did was wrong.

In my previous iteration I was adding information. I was assuming the velocity (or acceleration) reached a particular value at a particular point---this was a new assumption. The previous information was only about averages.

\section{Statistical test for slope significance}

The most robust way I can think is to compute a linear fit to the data (or its higher derivatives) starting at an initial point, then keep including more and more points, while keeping a running student $t$-test of the significance of the slope. If the slope reaches a significant value then either stop at that point, or continue until its significance starts to decrease. Now either continue from the next point, or start at the end point. This way you create a piece-wise derivative

Wait. Why do a linear fit? Just do a mean at an initial point and see if its different from zero.

\section{Knot placement in B-splines}

A couple of different strategies.

The first is to place a knot at each observation point, and then use tension to adjust the fit. This is different from what I've done because knot placement would be variable.

Another technique is to use the NEWNOT function in Practical Guide to Splines to choose knot locations based on a high order derivative. In this case the free parameter would be the number of knots.

The final technique to try is that by Jerome Blair

\section{Assessing a good fit}

Using autocorrelation and a the Box-Ljung test to assess whether or not the errors appear uncorrelated. This works well, until our periodic signal fouls things up. In particular, we have a signal with a period of 7 hours, which corresponds to 14 data points. In the autocorrelation sequence for the noise, we start to see strong correlation at 7 and again at 14 data points. This makes sense because the tension will always work to reign in oscillations (deviations from some straight line).

\section{Proper normalization}

I've always struggled to get the proper normalization for the tension term, but I figured out the issue. We normally have that,
\begin{equation}
\phi =  \sum_{i=1}^{N} \frac{1}{2} \left( \frac{x_i - x(t)}{\sigma_i} \right) ^2 +  \frac{1}{\bar{u}^2 T} \int \dot{x}^2(t) \, dt.
\end{equation}
but what I missed before was that the first term scales like $N$, the number of data points. So what you really want is,
\begin{equation}
\phi =  \sum_{i=1}^{N} \frac{1}{2} \left( \frac{x_i - x(t)}{\sigma_i} \right) ^2 +  \frac{N}{\bar{u}^2 T} \int \dot{x}^2(t) \, dt.
\end{equation}

\section{Knot selection}

We want more knots in regions of lots of variation, and we need at least one datapoint in for each spline.

In PGS Chapter 12, de Boor argues that equal partitioning of the function,
\[
j(t) = \int_a^b |D^k g(t) |^{1/k} dt
\]
makes for good knot placement. I have found that this does not converge to a fixed set of knot points after repeated application using the same function. Furthermore, it can cause a spline to be defined between data points.

So we'd need a way of choosing knot points such that we can guarantee that no spline will be created without support from data. Given data sites $\tau_i$, you could choose knot points at
\[
t_{k+i} = \frac{\tau_{i+1} + ... + \tau_{i+k-1}}{k-1},\,i=1,...n-k
\]
just as in A Practical Guide to Splines, Chapter 13, equation 28. This is a nice data-centric way of choosing points.

So let's combine these two ideas and do a weighted selection of spline points, such as
\[
t_{k+i} = \frac{j(\tau_{i+1}) \tau_{i+1} + ... + j(\tau_{i+k-1}) \tau_{i+k-1}}{j(\tau_{i+1}) + ... + j(\tau_{i+k-1})},\,i=1,...n-k
\]
I think we can easily show that you can't create a spline without any data support.

We could take two different philosophies when deciding the knot points. Either we want each spline to have the same amount of data support, on average, or we want each datum to have the same number of nonzero splines. The latter is the default philosophy of the usual construction. So let's assume that.

There's a natural weighting here too. The splines sum to 1 at each point. That's the opposite feature that we want. We 

You could also weight by the integrated value of $j(t)$ around the data point.

Using our technique of increasing the multiplicity of the end point knots, if we start with $N$ observations and use each observation as a knot point, we will have $M=N+2(K-1)$ knots, which will produce $M=N+K-2$ splines. This makes for $N-2$ interior knots, which I suppose is super obvious.


$M$ knot points, with a spline of order $K$, produce $M-K$ splines. The $K$-th spline is the has the first knot at its left-most boundary and the $K+K+1$ knot as it's right-most boundary, where the spline is zero. It therefore provides non-zero support at $S=K-1$ knot points. The first spline only provides support for the initial knot point. The second splines only provides support for the second knot point. However, these maybe best thought of as providing support for the first interval, first and second interval, etc.

$M$ knot points for $M$ data points with splines of order $K=4$ should provide support for,

\begin{tabular}{cc}
spline & \# of data \\
1 & 1 \\
2 & 1 \\
3 & 2 \\
4 & 3 \\
5 & 3 \\
\vdots & \vdots \\
M-4 & 3 \\
M-3 & 3 \\
M-2 & 2 \\
M-1 & 1 \\
M & 1
\end{tabular}


So, the best way to think of this is the number of intervals that are supported by each spline.

But maybe we need to flip this around. The first knot is fixed at the end point. The second knot provides non-zero support for three splines. This is always true. 

We can think of knot points as being created by a filter that takes the data positions as input. The center of the $i$-th filter is denoted $\tau^c_i$ and has width $w$. I suppose that $\tau^c_i= \frac{N}{M} i$ where $N$ is the number of data points and $M$ is the number of knot points. This is fractional. The width of the filter would probably be $w=\frac{N}{M}(K-2)$, where the width of the filter decreases (symmetrically) as the filter approaches the boundary.

So how do you deal with fractional indices? I think the answer is that you weight the values by that fraction. So if we're including indices $1:3.25$, then we include index 4 as well, 


I'm now weighting by data density, so that we don't get knot points in regions of no data. One could also weight by data quality, as determined by the error in the previous iteration. I think this is necessary because the spline technique needs a way of `rejecting' bad data points. In other words, the method of adding knot points without tension doesn't work well for non-Gaussian data.

So how do we set the tension? First, looking at the variance of the errors coming out, they are very robustly $160^2$ meters squared. For a student t-distribution, this suggests $\nu=2.018$ if we take $\sigma=15$, or $\nu=2.008$ if we take $\sigma=10$.

The quantity,
\[
\phi = \frac{1}{2} \epsilon_k  W\indices{^k_i} \epsilon^i 
\]
is the tricky quantity to determine. If the errors are Gaussian, then it scales like $N$. However, in our case we've re-weighted the $\sigma$ associated with each data point so that it resembles a t-distribution. 

In my tests, for a spline $S=5$, I find that $2\phi/N \sim 1/2400$, while for a spline $S=4$ I find that $2\phi/N \sim 1/1200$ and  $S=3$ I find that $2\phi/N \sim 1/950$. It's hard to know what's going on here. Assuming the errors themselves remain the same (they might not!), then this is suggesting that the weights $\sigma_i^2$ for each error are growing larger. Conversely, this suggest that the errors are getting smaller if the weights remain the same.

\section{Assessing a good fit, revisited}
I see three possibilities.

First, we use the Ljung-Box test on the entire ensemble of autocorrelation sequences.

Second, we ask for consistency in the parameters for the error distribution.

Third, we want the autocorrelation sequence of the velocity not to go negative at lag 1, although I think this follows from above.

Need to discuss cross-validation as a technique.

\section{Notes to self}

I have to go back to the idea of limiting the velocity distribution, not the time integral of it. That makes way more sense.

\section{Velocity probability function}

If we want to do a maximum likelihood of this,
\begin{equation}
P \sim \prod^N \exp \left[ -\frac{1}{2} \left( \frac{x_i - x(t_i)}{\sigma_i} \right)^2 \right] \cdot \prod^{N_u} \exp \left[  - \frac{1}{2} \left(  \frac{\dot{x}}{\sigma_u} \right)^2 \right] \Delta x.
\end{equation}
then we want to minimize this,
\begin{equation}
\phi = \frac{1}{2 N} \left[ x^k - X\indices{^k_j} \xi^j \right]^{\textrm{T}} \left(\Sigma^{-1}\right)\indices{^k_i} \left[ x^i - X\indices{^i_l} \xi^l \right] + \frac{1}{2 N_u} \left[V\indices{^k_j} \xi^j \right]^{\textrm{T}} W\indices{^k_i} \left[ V\indices{^i_j} \xi^j \right].
\end{equation}
The 1/N normalizations are justified on grounds that we're looking for an average likelihood, something not dependent on the number of observations or velocity grid.

Differentiating the parameters to find the minimum we find that,
\begin{align}
\frac{\partial \phi}{\partial \xi^m} = - \frac{2}{N} x_k \left(\Sigma^{-1}\right)\indices{^k_i}  X\indices{^i_m}+ \frac{2}{N} X\indices{_k_m} \left(\Sigma^{-1}\right)\indices{^k_i}  X\indices{^i_l} \xi^l + \frac{2}{N_u} V\indices{_k_m} W\indices{^k_i}  V\indices{^i_l} \xi^l
\end{align}
which looks like it will give us exactly the same solution as the tension constraint, assuming a fine grid.

\section{OMG I HAVE THE ANSWER}

All my work that I did classifying the noise of the velocity and the acceleration IS the freakin' answer.

Basically, you set the tension based on the noise of each derivative. This will smooth different amounts at each derivative. (since we just showed it's a Gaussian probability function) Let's say we have an expected noise of 1 m/s, then we'd want to restrict our fit to velocities below that value. Because, intuitively, if we think we're observing something with 1 m/s velocity, then it's likely? just noise.

We could pick a sensible value. 95\% of the noise is within $2\sigma$. 68\% of the noise is within $\sigma$. So we can keep shrinking until we get to 5\% of the noise. This would mean that 95\% of the velocity we're observing is signal. That number is $0.0627 \sigma$, using
\[
\textrm{Pr}\sim1-\left(1-\frac{1}{2}\left(1+\textrm{erf} \left( \frac{x}{\sigma \sqrt{2}} \right) \right)2 \right)
\]

What happens if you double the distance between points? For the velocity variance, this drops it by a factor of two. Meaning, that you'd now have to consider lower velocities as improbable. 

This needs to be modified by removing the mean---or actually just subtracting the fit from the observations.

When I place a velocity tension condition, I cannot have a particle that starts out with a non-zero velocity. This really does act like tension and bends the velocity curve to fit. So, place an acceleration tension condition, then it works. Same is true at the next order. If you start out with non-zero forcing, you're going to want to place jerk conditions.

So what's happening here is that if you demand a velocity tension, essentially you're asking to minimized the total squared velocity, while trading off with how much you're going to miss the observation points. So why does it reduce the velocity at the beginning and end, and yet match it while in the middle? 


\section{Further refined penalty function}
So we're going to have a penalty function like this,
\begin{equation}
\phi =  \frac{1}{2} \left[I\indices{^q_j} \left[ \mathcal{D}^2\right]\indices{^j_k} x^k - A\indices{^q_j} \xi^j \right]^{\textrm{T}} {\Sigma^{-1}}\indices{^k_i} \left[I\indices{^q_j} \left[\mathcal{D}^2\right]\indices{^j_i} x^i - A\indices{^q_j} \xi^j \right] 
\end{equation}
where $I\indices{^k_q}$ is the integration function and the index $q$ is the integration grid. The point here being that instead of requiring the \emph{average} velocity or acceleration to agree over the interval between appropriate observations, we are requiring it to agree at all points between the observation. When I say agree, I mean reproduce the expected noise.

Perhaps this tension is only needed on the highest non-zero derivative? Yes. The only additional thing that would be solved by including it on the other derivatives would be an integration constant....which is what using the positions will take care of.

Let's say we have really good accuracy in position and relatively weak temporal sampling. In other words, we know the average velocity between two points with good accuracy, but we've under sampled. If we constrain the velocity of our fit using the above method, then a true constant acceleration trajectory would reveal an infinite acceleration at each point. In other words, we'd have give ourselves enough information for a spline of order 1. You see, this will keep the velocities near the average velocity, and at the arc in the parabola it will necessarily have a discontinuous second derivative. Now, if we'd constrained the acceleration, it would produce a constant acceleration, provided we haven't also tried to constrain the velocity. This is all true in the under sampled case.

But what if we're in the over sampled case? Now we have too much noise too really determine an acceleration. If we apply this technique to the acceleration, then it should give us something indistinguishable from zero. But if we use our technique of averaging the deviation, then it seems to me that it might allow acceleration to become really sloppy. I mean that high variation around zero would also produce the same level of noise. SOOOO, this would be a case where you'd want to it to be constant in this range..



\section{Spline placement algorithm}

Compute the derivatives of the observations and the expected noise at each derivative. When the rms value of the derivative exceeds the expected noise, add a new knot. You do this for the order of the spline that you're interested in.

So, pick an order for a spline. Say, $K=3$, $S=2$. This is a spline that has a constant second derivative, acceleration. Now Compute the acceleration and the expected noise for acceleration at each point. Find all the points that exceed the $2\sigma$ noise threshold. Then we need to start asking if my neighbor is significantly different from me. If it is, we want a new knot, if it's not, we don't.

I'm not quite sure how to get around the correlation structure. 

Student t-test is,
\begin{equation}
t = \frac{ \bar{x} - \mu}{s/\sqrt{n}}
\end{equation}
where $s$ is the sample variance, not the expected variance. But, in our case we know the variance, so we use the z-test,
\begin{equation}
z = \frac{ \bar{x} - \mu}{\sigma/\sqrt{n}}
\end{equation}
where $z>2$ is 95\% confidence.

What do we use for the variance?
\begin{align}
\textrm{var}(A+B+C) =& \textrm{var}(A) + \textrm{var}(B+C) + 2\textrm{cov}(A,B+C) \\
=& \textrm{var}(A) +\textrm{var}(B) + \textrm{var}(C)  + 2\textrm{cov}(A,B)  + 2\textrm{cov}(A,C) + 2\textrm{cov}(B,C) 
\end{align}
Which means that we sum the columns on the covariance matrix for the values that we're interested in. This actually reduces the variances to just the first and last (the others cancel). The weird thing is that we're then 

Unequal sample sizes from the t-test,
\begin{equation}
z = \frac{ \bar{x}_1 - \bar{x}_2}{ \sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}} } 
\end{equation}

This appears to be more conservative than just throwing out data and extending the time interval. That's not right.

\section{References}

Check out `The Rod-Spring approximation: An intuitive approach to the best-fit least-squares linear approximation'.

%We proceed by picking a parametric model for $x(t), y(t)$, then actually go and compute how the coefficients need to be minimized. For example,
%\begin{equation}
%x(t) = at^3 + bt^2 + ct +d
%\end{equation}
%so,
%\begin{equation}
%u(t) = \frac{a}{3}t^2 + \frac{b}{2}t + c
%\end{equation}
%and therefore,
%\begin{equation}
%\int_{t_0}^{t_1} u^2(t) = \frac{a}{3}\left(t_1^2 - t_0^2 \right) + \frac{b}{2} \left(t_1 - t_0 \right) + c
%\end{equation}
%Proceeding in this way, we can write down what the action, $S$, is for this particular model between the two time points.
%
%
%
%One other order of business is that we can directly relate the holonomic constraints to the maximum likelihood model. Still need to do that.

%Now consider equation \ref{constrained_lagrangian}, but without the velocity potential. That was, 
%\begin{equation}
%\mathcal{L}=\frac{1}{2} m \dot{x}^2 + \sum_{i=0}^N \lambda^{(i)}(x_i-x(t_i)).
%\end{equation}
%Inserting this into our exponential,
%\begin{align}
%P \sim& \exp \left[ \frac{S}{E} \right] \\
%\sim& \exp \left[ \int \left( \frac{1}{2} m \dot{x}^2 + \sum_{i=0}^N \lambda^{(i)}(x_i-x(t_i)) \right) \, dt \right]
%\end{align}
%
%The principal of least action does \emph{not} say that we want to minimized velocities, 
%
%- One could assume that tangential and normal accelerations are slowly varying.
%- One could assume 

%Reference on GPS errors,
%	http://www.leb.esalq.usp.br/disciplinas/Molin/leb447/Arquivos/GNSS/ArtigoAcuraciaGPSsemAutor.pdf

\end{document}  